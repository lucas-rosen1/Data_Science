{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will use numpy to build logistic regression, the simplest classification algorithm.\n",
    "\n",
    "We can also think of logistic regression as a nueral network with only 1 nueron.\n",
    "\n",
    "For more information on the intuition behind Logistic Regression please see Logistic_Regression.pptx in Week_1/Class_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "#Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "#don't worry about matplot yet, I'll do the plotting for you.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "#Just take first two features to make it easier to plot\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "#Just take class 0 and 1\n",
    "X = X[y!=2]\n",
    "y = y[y!=2]\n",
    "\n",
    "\n",
    "#These would be hyperparameters in your logistic regression that I have set for you\n",
    "#Feel free to play around with these after completing the assignment\n",
    "alpha = 0.01\n",
    "num_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the two classes so we can see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = X[y==0]\n",
    "X1 = X[y==1]\n",
    "plt.plot(X0[:,0],X0[:,1],'o',label='Class 0')\n",
    "plt.plot(X1[:,0],X1[:,1],'o',label='Class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly imagine a linear boundary between the two classes. Thus, logistic regression should work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task:\n",
    "I will present you the code behind logistic regression using loops and no vectorization. You will time this code. You will them remove the loops layer by layer and see how the speed increases each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 0: All loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #Sigmoid Function\n",
    "    #Create sigmoided list element by element\n",
    "    sig = []\n",
    "    for ele in z:\n",
    "        sigi = 1  / (1+ np.exp(-ele))\n",
    "        sig.append(sigi)\n",
    "    return sig\n",
    "\n",
    "def loss(h, y):\n",
    "    #Cross Entropy Loss\n",
    "    \n",
    "    #Create loss list element by element\n",
    "    \n",
    "    loss_list = []\n",
    "    for i in range(len(y)):\n",
    "        loss_i = (-y[i] * math.log(h[i])) - (1-y[i]) * math.log(1-h[i])\n",
    "        loss_list.append(loss_i)\n",
    "    \n",
    "    #Get sum of loss_list\n",
    "    tot = 0\n",
    "    for ele in loss_list:\n",
    "        tot+=ele\n",
    "    \n",
    "    #Return mean of loss_list by dividing tot by number of elements in it\n",
    "    return tot/len(loss_list)\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            for j in range(X.shape[1]):\n",
    "                grad += X[i,j]*theta[j]\n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            grad_ele = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                grad_ele += X[i,j]*(h-y)[i]\n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        for i in range(len(gradient)):\n",
    "            gradient[i]/=len(y)\n",
    "        \n",
    "        #Update theta\n",
    "        for i in range(len(theta)):\n",
    "            theta[i] -= alpha * gradient[i]\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        grad = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            grad += X[i,j]*theta[j]\n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    \n",
    "    #Get predictions by comparing element by element with .5\n",
    "    probs = predict_proba(X,theta)\n",
    "    \n",
    "    preds = []\n",
    "    for ele in probs:\n",
    "        pred = int(ele >=.5)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    \n",
    "    #Check equality element by element and add to a list. 1 if equal, 0 if not equal\n",
    "    equals_list = []\n",
    "    for i in range(len(y)):\n",
    "        equals = int(y[i] == y_pred[i])\n",
    "        equals_list.append(equals)\n",
    "    \n",
    "    #Sum the elements of equals list and then return the mean\n",
    "    tot = 0\n",
    "    for ele in equals_list:\n",
    "        tot+=ele\n",
    "    return tot/len(equals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets plot our decision boundary with the data\n",
    "plt.plot(X0[:,0],X0[:,1],'o',label='Class 0')\n",
    "plt.plot(X1[:,0],X1[:,1],'o',label='Class 1')\n",
    "x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs = np.array(predict_proba(grid,theta)).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, [0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: Get Rid of Basic Loops\n",
    "You will get rid of the loops in the helper functions `sigmoid`, `loss`, `predict`, and `accuracy`, and some of the loops in `fit`\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "    \n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            for j in range(X.shape[1]):\n",
    "                grad += X[i,j]*theta[j]\n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            grad_ele = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                grad_ele += X[i,j]*(h-y)[i]\n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 to 2 LINES\n",
    "        #HINT: FIRST CAST `gradient` TO AN ARRAY, THEN DIVIDE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        grad = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            grad += X[i,j]*theta[j]\n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2: Get Rid of Inner Loops\n",
    "\n",
    "Get rid of the inner loops in `fit` and `predict_proba`\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            \n",
    "            #WRITE CODE HERE\n",
    "            #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "            ####################################\n",
    "            \n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            \n",
    "            #WRITE CODE HERE\n",
    "            #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "            #HINT: TO ACCESS THE jth COLUMN OF X WRITE X[:,j]\n",
    "\n",
    "            ####################################\n",
    "            \n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS IS THE SAME CODE AS FOR THE FIRST INNER LOOP IN `fit`\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 3: No Loops (Except in Iterations)\n",
    "You will get ride of the outerloop in `fit` and `predict proba`\n",
    "This will confer a huge time save.\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        \n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "        #HINT: You'll have to use the axis argument in \n",
    "        #https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.sum.html\n",
    "\n",
    "        ####################################\n",
    "\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        #WRITE CODE HERE\n",
    "        #HINT: YOU'LL AGAIN HAVE TO USE THE AXIS ARGUMENT IN SUM\n",
    "        #HINT2: TO TURN AN (n,) array to a (n,1) array use <ARR>[:,None]\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector    \n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS IS THE SAME CODE AS FOR THE FIRST LOOP IN `fit`\n",
    "\n",
    "    ####################################\n",
    "        \n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4: More Time Saves (Bonus)\n",
    "\n",
    "For a Bonus 2 points, replace some of the sums with dot products. This is even faster.\n",
    "\n",
    "Write code where it says \n",
    "`#REPLACE WITH DOT PRODUCT`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        \n",
    "        #REPLACE WITH DOT PRODUCT\n",
    "        #HINT: A @ B denotes the dot product of A and B\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        #REPLACE WITH DOT PRODUCT\n",
    "        #HINT: TO TAKE THE TRANPOSE OF MATRIX A, USE A.T\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS IS THE SAME CODE AS FOR THE DOT PRODUCT IN `fit`\n",
    "\n",
    "    ####################################\n",
    "        \n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#Should be .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The way the pros implement it (No code to write, just observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X)\n",
    "#Check accuracy\n",
    "accuracy(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets time it\n",
    "clf = LogisticRegression()\n",
    "%timeit clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets mem it\n",
    "clf = LogisticRegression()\n",
    "%memit clf.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
