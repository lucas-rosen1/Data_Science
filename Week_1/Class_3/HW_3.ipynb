{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will use numpy to build logistic regression, the simplest classification algorithm.\n",
    "\n",
    "We can also think of logistic regression as a nueral network with only 1 nueron.\n",
    "\n",
    "For more information on the intuition behind Logistic Regression please see Logistic_Regression.pptx in Week_1/Class_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:38:21.191976Z",
     "start_time": "2019-01-11T20:38:21.158026Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:38:24.022504Z",
     "start_time": "2019-01-11T20:38:23.252318Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "#Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "#don't worry about matplot yet, I'll do the plotting for you.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "#Just take first two features to make it easier to plot\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "#Just take class 0 and 1\n",
    "X = X[y!=2]\n",
    "y = y[y!=2]\n",
    "\n",
    "\n",
    "#These would be hyperparameters in your logistic regression that I have set for you\n",
    "#Feel free to play around with these after completing the assignment\n",
    "alpha = 0.01\n",
    "num_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the two classes so we can see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:38:27.530680Z",
     "start_time": "2019-01-11T20:38:27.366103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHMlJREFUeJzt3X+QXFWd9/H3l8nozPIjIWRwYSZhNGxRSpInP0YilYLCsI/sQjYGghIUlmBMVl0EZAtd0FI2uoatVBEen7V2i5iS8EMwZiESHoVFfgirkodJyA8ggqBhkwFlCGQACXEyfPeP7kkyPT0zfab7Tt9z+/Oqmprp25cz33OuftNz7/ecY+6OiIhky2HVDkBERCpPyV1EJIOU3EVEMkjJXUQkg5TcRUQySMldRCSDlNxFRDKo5ORuZnVm9qSZ3VvkvYVm1mlmm/Nfn61smCIiEmJUwLlXANuBowZ4/4fufln5IYmISLlKSu5m1gKcA/wzcFUlfvG4ceO8tbW1Ek2JiNSMjRs3vuruTUOdV+on9xuBLwNHDnLOfDM7HXgO+JK77xyswdbWVtrb20v89SIiAmBmL5Zy3pD33M1sDvCKu28c5LT1QKu7TwF+BqweoK0lZtZuZu2dnZ2lxCciIsNQygPVWcBcM9sB3AnMNrPbDj3B3Xe7+778y5XAjGINuftN7t7m7m1NTUP+VSEiIsM0ZHJ392vcvcXdW4EFwEPuftGh55jZcYe8nEvuwauIiFRJSLVMH2a2FGh393uAy81sLrAfeA1YWJnwRCTLuru72bVrF++88061Q0mdhoYGWlpaqK+vH9Z/b9Vaz72trc31QFWktv3ud7/jyCOP5JhjjsHMqh1Oarg7u3fv5s033+T9739/n/fMbKO7tw3VxrA/uYvUinVPdrD8/md5ac9ejh/TyNVnncS8ac3VDisT3nnnHVpbW5XYC5gZxxxzDOUUnii5iwxi3ZMdXHPXNvZ29wDQsWcv19y1DUAJvkKU2Isrd1y0tozIIJbf/+yBxN5rb3cPy+9/tkoRiZRGyV1kEC/t2Rt0XOLz+9//ngULFjBx4kQ+9KEPcfbZZ/Pcc8+xY8cOJk2alMjv3LdvHxdccAEnnngiM2fOZMeOHRX/HUruIoM4fkxj0HFJ1ronO5h1/UO8/x//H7Ouf4h1T3aU1Z67c+6553LGGWfwwgsv8Mwzz/Dtb3+bP/zhDxWKuLhVq1Zx9NFH8/zzz/OlL32Jr3zlKxX/HUruIoO4+qyTaKyv63Ossb6Oq886qUoR1a7e5x8de/biHHz+UU6Cf/jhh6mvr+dzn/vcgWNTp07ltNNO63Pejh07OO2005g+fTrTp0/nl7/8JQAvv/wyp59+OlOnTmXSpEk89thj9PT0sHDhQiZNmsTkyZNZsWJFv9/74x//mEsuuQSA888/nwcffJBKVy7qgarIIHofmqpapvoGe/4x3Ovx1FNPMWNG0Qn1fRx77LE88MADNDQ08Jvf/IYLL7yQ9vZ2fvCDH3DWWWfx1a9+lZ6eHt5++202b95MR0cHTz31FAB79uzp115HRwfjx48HYNSoUYwePZrdu3czbty4YfWjGCV3kSHMm9asZJ4C1Xz+0d3dzWWXXcbmzZupq6vjueeeA+DDH/4wn/nMZ+ju7mbevHlMnTqVD3zgA/z2t7/li1/8Iueccw4f+9jH+rVX7FN6pauGdFtGRKKQxPOPk08+mY0bB1sTMWfFihW8733vY8uWLbS3t/OnP/0JgNNPP51HH32U5uZmLr74Ym655RaOPvpotmzZwhlnnMF3v/tdPvvZ/nsXtbS0sHNnbuHc/fv309XVxdixY4fdj2KU3EUkCkk8/5g9ezb79u1j5cqVB4498cQT/PznP+9zXldXF8cddxyHHXYYt956Kz09udtDL774IsceeyyLFy9m0aJFbNq0iVdffZV3332X+fPn881vfpNNmzb1+71z585l9erc4rlr165l9uzZFf/krtsyIhKFJJ5/mBl33303V155Jddffz0NDQ20trZy44039jnvC1/4AvPnz+dHP/oRH/3oRzn88MMBeOSRR1i+fDn19fUcccQR3HLLLXR0dHDppZfy7rvvArBs2bJ+v3fRokVcfPHFnHjiiYwdO5Y777xz2H0YsG9aW0ZEqmX79u188IMfrHYYqVVsfEpdW0a3ZUREMkjJXUQkg5TcRUQySMldRCSDlNxFRDJIpZCSGdpUQ+QgfXKXTEhiUSmpDdVY8vfRRx9l+vTpjBo1irVr1ybyO5TcJRO0qUaN2LoGVkyC68bkvm9dU1Zz1Vryd8KECdx888186lOfSux3KLlLJmhTjRqwdQ2svxy6dgKe+77+8rISfLWW/G1tbWXKlCkcdlhyKVj33CUTjh/TSEeRRK5NNTLkwaXQXXCNu/fmjk/55LCarNaSvyNBn9wlE7SpRg3o2hV2vIK6u7tZvHgxkydP5hOf+ATPPPMMkFvy9/vf/z7XXXcd27Zt48gjj+yz5O99993HUUcdlXh8xSi5SybMm9bMsvMm0zymEQOaxzSy7LzJqpbJktEtYcdLUK0lf0eCbstIZmhTjYw78+u5e+yH3pqpb8wdH6bZs2dz7bXXsnLlShYvXgzklvx9++23OeGEEw6c19XVRUtLC4cddhirV6/us+Rvc3Mzixcv5o9//CObNm3i7LPP5j3veQ/z589n4sSJLFy4cNjxlUPJXcqm+nIZEb331R9cmrsVM7oll9iHeb8dqrfk7xNPPMG5557L66+/zvr16/nGN77B008/Pex+FO2blvyVcvTWlx9ahthYX6dbIlISLfk7OC35K1Wj+nKRdFJyl7KovlwknZTcpSxJbFostaVat4bTrtxxUXKXsqi+XMrR0NDA7t27leALuDu7d++moaFh2G2oWkbKksSmxVI7Wlpa2LVrF52dndUOJXUaGhpoaRl+Db+qZUREIlJqtUzJn9zNrA5oBzrcfU7Be+8FbgFmALuBC9x9R1DEIhmhun9Jg5B77lcA2wd4bxHwurufCKwA/qXcwERipHXlJS1KSu5m1gKcA3xvgFM+DqzO/7wWONPMrPzwROKiun9Ji1I/ud8IfBl4d4D3m4GdAO6+H+gCjik8ycyWmFm7mbXrAYpkker+JS2GTO5mNgd4xd0HWzqt2Kf0fk9q3f0md29z97ampqaAMEXioLp/SYtSPrnPAuaa2Q7gTmC2md1WcM4uYDyAmY0CRgOvVTBOkSio7l/SYsjk7u7XuHuLu7cCC4CH3P2igtPuAS7J/3x+/hzNSpCao3XlJS2GPYnJzJYC7e5+D7AKuNXMnif3iX1BheITiY7WlZc0CEru7v4I8Ej+568fcvwd4BOVDEzka+u2cceGnfS4U2fGhTPH8615k6sdlkgUtPyApNLX1m3jtsf/+8DrHvcDr5XgRYamhcMkle7YsDPouIj0peQuqdQzwPP4gY6LSF9K7pJKdQNMcB7ouIj0peQuqXThzPFBx0WkLz1QlVTqfWiqahmR4dF67iIiESl1PXfdlhERySDdlpGiPr3yV/zihYPLA82aOJbbF59axYiqR5tvSIz0yV36KUzsAL944TU+vfJXVYqoerT5hsRKyV36KUzsQx3PMm2+IbFSchcZhDbfkFgpuYsMQptvSKyU3KWfWRPHBh3PMm2+IbFScpd+bl98ar9EXqvVMtp8Q2KlSUwiIhEpdRKT6tylqKRqu0PaVX25yPApuUs/vbXdvSWAvbXdQFnJNaTdpGIQqRW65y79JFXbHdKu6stFyqPkLv0kVdsd0q7qy0XKo+Qu/SRV2x3SrurLRcqj5C79JFXbHdKu6stFyqMHqtJP7wPLSleqhLSbVAwitUJ17iIiEVGde4JirL+OMWYRGT4l90Ax1l/HGLOIlEcPVAPFWH8dY8wiUh4l90Ax1l/HGLOIlEfJPVCM9dcxxiwi5VFyDxRj/XWMMYtIefRANVCM9dcxxiwi5VGdu4hIRCpW525mDcCjwHvz5691928UnLMQWA505A/9q7t/LzRoSc7X1m3jjg076XGnzowLZ47nW/MmV+T8tNTQpyUOkTQo5bbMPmC2u79lZvXAf5nZT9398YLzfujul1U+RCnX19Zt47bH//vA6x73A6+LJeyQ89NSQ5+WOETSYsgHqp7zVv5lff6rOvdyZFju2LAzseNpqaFPSxwiaVFStYyZ1ZnZZuAV4AF331DktPlmttXM1prZ+AHaWWJm7WbW3tnZWUbYEqJngOcqlTielhr6tMQhkhYlJXd373H3qUALcIqZTSo4ZT3Q6u5TgJ8Bqwdo5yZ3b3P3tqampnLilgB1ZokdT0sNfVriEEmLoDp3d98DPAL8VcHx3e6+L/9yJTCjItFJRVw4s+gfUhU5npYa+rTEIZIWQyZ3M2syszH5nxuBvwR+XXDOcYe8nAtsr2SQUp5vzZvMRR+ZcOCTd50ZF31kwoDVLyHnz5vWzLLzJtM8phEDmsc0suy8ySP+EDMtcYikxZB17mY2hdxtljpy/xiscfelZrYUaHf3e8xsGbmkvh94Dfi8u/96wEZRnbuIyHCUWueuSUwiIhHRZh0JSnKyTOhko6TaDeljUuOR1FhEa+saeHApdO2C0S1w5tdhyierHZWklJJ7oCQny4RONkqq3ZA+JjUeSY1FtLaugfWXQ3e+tLNrZ+41KMFLUVoVMlCSk2VCJxUl1W5IH5Maj6TGIloPLj2Y2Ht1780dFylCyT1QkpNlQicVJdVuSB+TGo+kxiJaXbvCjkvNU3IPlORkmdBJRUm1G9LHpMYjqbGI1uiWsONS85TcAyU5WSZ0UlFS7Yb0ManxSGosonXm16G+4B/M+sbccZEi9EA1UJIbX/Q+KKx0hUhouyF9TGo8khqLaPU+NFW1jJRIde4iIhFRnbv0EVqLro0vZECqt4+CknsNCK1F18YXMiDV20dDD1RrQGgtuja+kAGp3j4aSu41ILQWXRtfyIBUbx8NJfcaEFqLro0vZECqt4+GknsNCK1F18YXMiDV20dDD1RrQGgtepK1/BI51dtHQ3XuIiIRUZ07ydVqh7abhnXJVbeeUlmvGc96/0KN4HhkNrknVasd2m4a1iVX3XpKZb1mPOv9CzXC45HZB6pJ1WqHtpuGdclVt55SWa8Zz3r/Qo3weGQ2uSdVqx3abhrWJVfdekplvWY86/0LNcLjkdnknlStdmi7aViXXHXrKZX1mvGs9y/UCI9HZpN7UrXaoe2mYV1y1a2nVNZrxrPev1AjPB6ZfaCaVK12aLtpWJdcdesplfWa8az3L9QIj4fq3EVEIqI69wQlWTMe0vanV/6KX7zw2oHXsyaO5fbFp1YkDpFMufcq2HgzeA9YHcxYCHNuqEzbKa3lz+w996T01ox37NmLc7BmfN2THSPadmFiB/jFC6/x6ZW/KjsOkUy59ypoX5VL7JD73r4qd7xcvbXrXTsBP1i7vnVN+W2XSck9UJI14yFtFyb2oY6L1KyNN4cdD5HiWn4l90BJ1oyrHl0kAd4TdjxEimv5ldwDJVkzrnp0kQRYXdjxECmu5VdyD5RkzXhI27Mmji3axkDHRWrWjIVhx0OkuJZfyT3QvGnNLDtvMs1jGjGgeUwjy86bXJFqmZC2b198ar9ErmoZkSLm3ABtiw5+Ure63OtKVMtM+ST8zXdg9HjAct//5jupqJZRnbuISERKrXMf8pO7mTWY2f83sy1m9rSZ/VORc95rZj80s+fNbIOZtQ4vbBERqYRSJjHtA2a7+1tmVg/8l5n91N0fP+ScRcDr7n6imS0A/gW4oNLBhk4einGDipCNPUL6F+NYJDo5JGRSS5JxJNV2SifWVFRIH2thPAoMmdw9d9/mrfzL+vxX4b2cjwPX5X9eC/yrmZlX8J5P6IYTMW5QEbKxR0j/YhyLRDc26J3U0qt3Ugv0T/BJxpFU27WwSUZIH2thPIoo6YGqmdWZ2WbgFeABd99QcEozsBPA3fcDXcAxlQw0dPJQjBtUhGzsEdK/GMci0ckhIZNakowjqbZTPLGmYkL6WAvjUURJyd3de9x9KtACnGJmkwpOKbY4eb9P7Wa2xMzazay9s7MzKNDQCT4xTggK2dgjpH8xjkWik0NCJrUkGUdSbad4Yk3FhPSxFsajiKBSSHffAzwC/FXBW7uA8QBmNgoYDfSbB+/uN7l7m7u3NTU1BQUaOsEnxglBIRt7hPQvxrFIdHJIyKSWJONIqu0UT6ypmJA+1sJ4FFFKtUyTmY3J/9wI/CXw64LT7gEuyf98PvBQJe+3Q/jkoRg3qAjZ2COkfzGORaKTQ0ImtSQZR1Jtp3hiTcWE9LEWxqOIUqpljgNWm1kduX8M1rj7vWa2FGh393uAVcCtZvY8uU/sCyodaOiGEzFuUBGysUdI/2Ici0Q3Nuh9aFpKtUyScSTVdi1skhHSx1oYjyI0iUlEJCLarINIa7tlZMRYI51kzDHW26fluqRUZpN7lLXdMjJirJFOMuYY6+3Tcl1SLLMLh0VZ2y0jI8Ya6SRjjrHePi3XJcUym9yjrO2WkRFjjXSSMcdYb5+W65JimU3uUdZ2y8iIsUY6yZhjrLdPy3VJscwm9yhru2VkxFgjnWTMMdbbp+W6pFhmk3uSm2pI5EI2WEjLZgxJxpxUH5Mcu7RclxRTnbuISERU5y5SKSFrv6dFjDGnpW49LXGUScldZDAha7+nRYwxp6VuPS1xVEBm77mLVETI2u9pEWPMaalbT0scFaDkLjKYkLXf0yLGmNNSt56WOCpAyV1kMCFrv6dFjDGnpW49LXFUgJK7yGBC1n5PixhjTkvdelriqAAld5HBzLkB2hYd/NRrdbnXaX0wCXHGnJa69bTEUQGqcxcRiYjq3GXkxFgXnGTMSdWYxzjOUjVK7lKeGOuCk4w5qRrzGMdZqkr33KU8MdYFJxlzUjXmMY6zVJWSu5QnxrrgJGNOqsY8xnGWqlJyl/LEWBecZMxJ1ZjHOM5SVUruUp4Y64KTjDmpGvMYx1mqSsldyhNjXXCSMSdVYx7jOEtVqc5dRCQipda565O7ZMfWNbBiElw3Jvd965rqtJtUHCIBVOcu2ZBUHXhou6pHl5TQJ3fJhqTqwEPbVT26pISSu2RDUnXgoe2qHl1SQsldsiGpOvDQdlWPLimh5C7ZkFQdeGi7qkeXlFByl2xIqg48tF3Vo0tKqM5dRCQiFatzN7PxZvawmW03s6fN7Ioi55xhZl1mtjn/pb9BRUSqqJQ69/3AP7j7JjM7EthoZg+4+zMF5z3m7nMqH6JURYwbQ4TEHGP/0kJjF4Uhk7u7vwy8nP/5TTPbDjQDhcldsiLGiTghMcfYv7TQ2EUj6IGqmbUC04ANRd4+1cy2mNlPzezkCsQm1RLjRJyQmGPsX1po7KJR8vIDZnYE8B/Ale7+RsHbm4AT3P0tMzsbWAf8RZE2lgBLACZMmDDsoCVhMU7ECYk5xv6lhcYuGiV9cjezenKJ/XZ3v6vwfXd/w93fyv/8E6DezMYVOe8md29z97ampqYyQ5fExDgRJyTmGPuXFhq7aJRSLWPAKmC7uxddlNrM/jx/HmZ2Sr7d3ZUMVEZQjBNxQmKOsX9pobGLRim3ZWYBFwPbzGxz/ti1wAQAd/934Hzg82a2H9gLLPBqFdBL+XofjMVUERESc4z9SwuNXTQ0iUlEJCKlTmLSeu4xU71xX/deBRtvBu/JbW83Y2H529uJRErJPVaqN+7r3qugfdXB195z8LUSvNQgLRwWK9Ub97Xx5rDjIhmn5B4r1Rv35T1hx0UyTsk9Vqo37svqwo6LZJySe6xUb9zXjIVhx0UyTsk9VtoUoq85N0DbooOf1K0u91oPU6VGqc5dRCQiqnMPtO7JDpbf/ywv7dnL8WMaufqsk5g3rbnaYVVOLdTE10If00DjHAUld3KJ/Zq7trG3O1dZ0bFnL9fctQ0gGwm+Fmria6GPaaBxjobuuQPL73/2QGLvtbe7h+X3P1uliCqsFmria6GPaaBxjoaSO/DSnr1Bx6NTCzXxtdDHNNA4R0PJHTh+TGPQ8ejUQk18LfQxDTTO0VByB64+6yQa6/tOdmmsr+Pqs06qUkQVVgs18bXQxzTQOEdDD1Q5+NA0s9UytbAGdy30MQ00ztFQnbuISERKrXPXbRmRWGxdAysmwXVjct+3romjbakK3ZYRiUGS9eWqXc8kfXIXiUGS9eWqXc8kJXeRGCRZX67a9UxScheJQZL15apdzyQld5EYJFlfrtr1TFJyF4lBkuv3a2+ATFKdu4hIRFTnLiJSw5TcRUQySMldRCSDlNxFRDJIyV1EJIOU3EVEMkjJXUQkg5TcRUQyaMjkbmbjzexhM9tuZk+b2RVFzjEz+46ZPW9mW81sejLhiohIKUr55L4f+Ad3/yDwEeDvzexDBef8NfAX+a8lwL9VNEopnzZjEKkpQyZ3d3/Z3Tflf34T2A4Ubi76ceAWz3kcGGNmx1U8Whme3s0YunYCfnAzBiV4kcwKuuduZq3ANGBDwVvNwM5DXu+i/z8AUi3ajEGk5pSc3M3sCOA/gCvd/Y3Ct4v8J/1WJDOzJWbWbmbtnZ2dYZHK8GkzBpGaU1JyN7N6con9dne/q8gpu4Dxh7xuAV4qPMndb3L3Nndva2pqGk68MhzajEGk5pRSLWPAKmC7u98wwGn3AH+br5r5CNDl7i9XME4phzZjEKk5o0o4ZxZwMbDNzDbnj10LTABw938HfgKcDTwPvA1cWvlQZdh6N114cGnuVszollxi12YMIpmlzTpERCKizTpERGqYkruISAYpuYuIZJCSu4hIBim5i4hkUNWqZcysE3ixKr98cOOAV6sdRIKy3j/Ifh/Vv/iV08cT3H3IWaBVS+5pZWbtpZQZxSrr/YPs91H9i99I9FG3ZUREMkjJXUQkg5Tc+7up2gEkLOv9g+z3Uf2LX+J91D13EZEM0id3EZEMqtnkbmZ1Zvakmd1b5L2FZtZpZpvzX5+tRozlMLMdZrYtH3+/FdqysKl5CX08w8y6DrmOUa1xbGZjzGytmf06v0H9qQXvR30NS+hf7NfvpENi32xmb5jZlQXnJHYNS1nyN6uuILcf7FEDvP9Dd79sBONJwkfdfaBa2kM3NZ9JblPzmSMVWAUN1keAx9x9zohFU1n/B7jP3c83s/cAf1bwfuzXcKj+QcTXz92fBaZC7sMk0AHcXXBaYtewJj+5m1kLcA7wvWrHUkXa1DzFzOwo4HRyG+Xg7n9y9z0Fp0V7DUvsX5acCbzg7oUTNxO7hjWZ3IEbgS8D7w5yzvz8n0lrzWz8IOellQP/aWYbzWxJkfezsKn5UH0EONXMtpjZT83s5JEMrkwfADqB7+dvH37PzA4vOCfma1hK/yDe61doAXBHkeOJXcOaS+5mNgd4xd03DnLaeqDV3acAPwNWj0hwlTXL3aeT+7Pv783s9IL3S9rUPOWG6uMmclO1/xfwf4F1Ix1gGUYB04F/c/dpwB+Bfyw4J+ZrWEr/Yr5+B+RvOc0FflTs7SLHKnINay65k9s2cK6Z7QDuBGab2W2HnuDuu919X/7lSmDGyIZYPnd/Kf/9FXL3+U4pOKWkTc3TbKg+uvsb7v5W/uefAPVmNm7EAx2eXcAud9+Qf72WXDIsPCfWazhk/yK/fof6a2CTu/+hyHuJXcOaS+7ufo27t7h7K7k/lR5y94sOPafgntdccg9eo2Fmh5vZkb0/Ax8Dnio4LepNzUvpo5n9uZlZ/udTyP3vffdIxzoc7v57YKeZnZQ/dCbwTMFp0V7DUvoX8/UrcCHFb8lAgtewlqtl+jCzpUC7u98DXG5mc4H9wGvAwmrGNgzvA+7O//9iFPADd7/PzD4HmdnUvJQ+ng983sz2A3uBBR7XrL0vArfn/6z/LXBpxq7hUP2L/fphZn8G/G/g7w45NiLXUDNURUQyqOZuy4iI1AIldxGRDFJyFxHJICV3EZEMUnIXEckgJXcRkQxSchcRySAldxGRDPofF5zfT3elx/UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X0 = X[y==0]\n",
    "X1 = X[y==1]\n",
    "plt.plot(X0[:,0],X0[:,1],'o',label='Class 0')\n",
    "plt.plot(X1[:,0],X1[:,1],'o',label='Class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly imagine a linear boundary between the two classes. Thus, logistic regression should work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task:\n",
    "I will present you the code behind logistic regression using loops and no vectorization. You will time this code. You will them remove the loops layer by layer and see how the speed increases each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 0: All loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:39:03.549877Z",
     "start_time": "2019-01-11T20:39:03.534596Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #Sigmoid Function\n",
    "    #Create sigmoided list element by element\n",
    "    sig = []\n",
    "    for ele in z:\n",
    "        sigi = 1  / (1+ np.exp(-ele))\n",
    "        sig.append(sigi)\n",
    "    return sig\n",
    "\n",
    "def loss(h, y):\n",
    "    #Cross Entropy Loss\n",
    "    \n",
    "    #Create loss list element by element\n",
    "    \n",
    "    loss_list = []\n",
    "    for i in range(len(y)):\n",
    "        loss_i = (-y[i] * math.log(h[i])) - (1-y[i]) * math.log(1-h[i])\n",
    "        loss_list.append(loss_i)\n",
    "    \n",
    "    #Get sum of loss_list\n",
    "    tot = 0\n",
    "    for ele in loss_list:\n",
    "        tot+=ele\n",
    "    \n",
    "    #Return mean of loss_list by dividing tot by number of elements in it\n",
    "    return tot/len(loss_list)\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            for j in range(X.shape[1]):\n",
    "                grad += X[i,j]*theta[j]\n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            grad_ele = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                grad_ele += X[i,j]*(h-y)[i]\n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        for i in range(len(gradient)):\n",
    "            gradient[i]/=len(y)\n",
    "        \n",
    "        #Update theta\n",
    "        for i in range(len(theta)):\n",
    "            theta[i] -= alpha * gradient[i]\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        grad = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            grad += X[i,j]*theta[j]\n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    \n",
    "    #Get predictions by comparing element by element with .5\n",
    "    probs = predict_proba(X,theta)\n",
    "    \n",
    "    preds = []\n",
    "    for ele in probs:\n",
    "        pred = int(ele >=.5)\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    \n",
    "    #Check equality element by element and add to a list. 1 if equal, 0 if not equal\n",
    "    equals_list = []\n",
    "    for i in range(len(y)):\n",
    "        equals = int(y[i] == y_pred[i])\n",
    "        equals_list.append(equals)\n",
    "    \n",
    "    #Sum the elements of equals list and then return the mean\n",
    "    tot = 0\n",
    "    for ele in equals_list:\n",
    "        tot+=ele\n",
    "    return tot/len(equals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:39:09.904950Z",
     "start_time": "2019-01-11T20:39:05.430758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6931471805599458\n",
      "iter: 1, loss: 0.6923520096418682\n",
      "iter: 2, loss: 0.6915851537400577\n",
      "iter: 3, loss: 0.6908413304095472\n",
      "iter: 4, loss: 0.6901162795878267\n",
      "iter: 5, loss: 0.6894065658676098\n",
      "iter: 6, loss: 0.6887094189049731\n",
      "iter: 7, loss: 0.6880226046531316\n",
      "iter: 8, loss: 0.6873443214948847\n",
      "iter: 9, loss: 0.6866731164751174\n",
      "iter: 10, loss: 0.6860078177527258\n",
      "iter: 11, loss: 0.6853474801364835\n",
      "iter: 12, loss: 0.6846913411730956\n",
      "iter: 13, loss: 0.684038785744212\n",
      "iter: 14, loss: 0.6833893175240366\n",
      "iter: 15, loss: 0.6827425359680869\n",
      "iter: 16, loss: 0.6820981177610898\n",
      "iter: 17, loss: 0.6814558018596896\n",
      "iter: 18, loss: 0.6808153774331694\n",
      "iter: 19, loss: 0.6801766741404611\n",
      "iter: 20, loss: 0.679539554290637\n",
      "iter: 21, loss: 0.6789039065218616\n",
      "iter: 22, loss: 0.678269640704555\n",
      "iter: 23, loss: 0.6776366838315805\n",
      "iter: 24, loss: 0.6770049767042187\n",
      "iter: 25, loss: 0.6763744712597941\n",
      "iter: 26, loss: 0.6757451284166605\n",
      "iter: 27, loss: 0.67511691633636\n",
      "iter: 28, loss: 0.6744898090221683\n",
      "iter: 29, loss: 0.6738637851888931\n",
      "iter: 30, loss: 0.6732388273514109\n",
      "iter: 31, loss: 0.672614921089604\n",
      "iter: 32, loss: 0.6719920544555356\n",
      "iter: 33, loss: 0.6713702174953515\n",
      "iter: 34, loss: 0.6707494018636909\n",
      "iter: 35, loss: 0.6701296005127034\n",
      "iter: 36, loss: 0.6695108074412451\n",
      "iter: 37, loss: 0.6688930174925966\n",
      "iter: 38, loss: 0.6682762261913235\n",
      "iter: 39, loss: 0.667660429611694\n",
      "iter: 40, loss: 0.6670456242715658\n",
      "iter: 41, loss: 0.666431807046796\n",
      "iter: 42, loss: 0.6658189751022163\n",
      "iter: 43, loss: 0.6652071258359595\n",
      "iter: 44, loss: 0.6645962568345658\n",
      "iter: 45, loss: 0.6639863658367682\n",
      "iter: 46, loss: 0.6633774507042928\n",
      "iter: 47, loss: 0.6627695093983099\n",
      "iter: 48, loss: 0.6621625399604376\n",
      "iter: 49, loss: 0.6615565404974311\n",
      "iter: 50, loss: 0.660951509168828\n",
      "iter: 51, loss: 0.6603474441769941\n",
      "iter: 52, loss: 0.659744343759093\n",
      "iter: 53, loss: 0.6591422061806098\n",
      "iter: 54, loss: 0.6585410297301361\n",
      "iter: 55, loss: 0.6579408127151608\n",
      "iter: 56, loss: 0.6573415534586787\n",
      "iter: 57, loss: 0.6567432502964605\n",
      "iter: 58, loss: 0.6561459015748494\n",
      "iter: 59, loss: 0.6555495056489873\n",
      "iter: 60, loss: 0.6549540608813881\n",
      "iter: 61, loss: 0.6543595656407853\n",
      "iter: 62, loss: 0.6537660183012097\n",
      "iter: 63, loss: 0.6531734172412409\n",
      "iter: 64, loss: 0.6525817608434132\n",
      "iter: 65, loss: 0.6519910474937312\n",
      "iter: 66, loss: 0.6514012755812857\n",
      "iter: 67, loss: 0.6508124434979436\n",
      "iter: 68, loss: 0.650224549638101\n",
      "iter: 69, loss: 0.6496375923984833\n",
      "iter: 70, loss: 0.6490515701779888\n",
      "iter: 71, loss: 0.6484664813775615\n",
      "iter: 72, loss: 0.6478823244000924\n",
      "iter: 73, loss: 0.6472990976503401\n",
      "iter: 74, loss: 0.6467167995348678\n",
      "iter: 75, loss: 0.6461354284619967\n",
      "iter: 76, loss: 0.6455549828417664\n",
      "iter: 77, loss: 0.6449754610859073\n",
      "iter: 78, loss: 0.6443968616078191\n",
      "iter: 79, loss: 0.6438191828225535\n",
      "iter: 80, loss: 0.6432424231468034\n",
      "iter: 81, loss: 0.642666580998895\n",
      "iter: 82, loss: 0.6420916547987835\n",
      "iter: 83, loss: 0.6415176429680486\n",
      "iter: 84, loss: 0.6409445439298957\n",
      "iter: 85, loss: 0.6403723561091559\n",
      "iter: 86, loss: 0.639801077932288\n",
      "iter: 87, loss: 0.6392307078273809\n",
      "iter: 88, loss: 0.6386612442241584\n",
      "iter: 89, loss: 0.6380926855539822\n",
      "iter: 90, loss: 0.6375250302498571\n",
      "iter: 91, loss: 0.636958276746436\n",
      "iter: 92, loss: 0.6363924234800261\n",
      "iter: 93, loss: 0.6358274688885915\n",
      "iter: 94, loss: 0.6352634114117638\n",
      "iter: 95, loss: 0.6347002494908415\n",
      "iter: 96, loss: 0.6341379815688023\n",
      "iter: 97, loss: 0.6335766060903042\n",
      "iter: 98, loss: 0.633016121501694\n",
      "iter: 99, loss: 0.6324565262510125\n",
      "iter: 100, loss: 0.6318978187879997\n",
      "iter: 101, loss: 0.6313399975641021\n",
      "iter: 102, loss: 0.6307830610324775\n",
      "iter: 103, loss: 0.6302270076480014\n",
      "iter: 104, loss: 0.629671835867272\n",
      "iter: 105, loss: 0.6291175441486182\n",
      "iter: 106, loss: 0.628564130952102\n",
      "iter: 107, loss: 0.6280115947395272\n",
      "iter: 108, loss: 0.6274599339744428\n",
      "iter: 109, loss: 0.6269091471221511\n",
      "iter: 110, loss: 0.6263592326497103\n",
      "iter: 111, loss: 0.6258101890259418\n",
      "iter: 112, loss: 0.625262014721436\n",
      "iter: 113, loss: 0.6247147082085555\n",
      "iter: 114, loss: 0.6241682679614435\n",
      "iter: 115, loss: 0.6236226924560265\n",
      "iter: 116, loss: 0.6230779801700207\n",
      "iter: 117, loss: 0.6225341295829366\n",
      "iter: 118, loss: 0.6219911391760861\n",
      "iter: 119, loss: 0.6214490074325827\n",
      "iter: 120, loss: 0.6209077328373533\n",
      "iter: 121, loss: 0.6203673138771372\n",
      "iter: 122, loss: 0.6198277490404946\n",
      "iter: 123, loss: 0.6192890368178093\n",
      "iter: 124, loss: 0.6187511757012956\n",
      "iter: 125, loss: 0.6182141641850009\n",
      "iter: 126, loss: 0.617678000764812\n",
      "iter: 127, loss: 0.6171426839384593\n",
      "iter: 128, loss: 0.61660821220552\n",
      "iter: 129, loss: 0.6160745840674255\n",
      "iter: 130, loss: 0.6155417980274632\n",
      "iter: 131, loss: 0.6150098525907827\n",
      "iter: 132, loss: 0.6144787462643982\n",
      "iter: 133, loss: 0.6139484775571955\n",
      "iter: 134, loss: 0.6134190449799349\n",
      "iter: 135, loss: 0.6128904470452541\n",
      "iter: 136, loss: 0.6123626822676758\n",
      "iter: 137, loss: 0.6118357491636076\n",
      "iter: 138, loss: 0.6113096462513494\n",
      "iter: 139, loss: 0.6107843720510963\n",
      "iter: 140, loss: 0.6102599250849429\n",
      "iter: 141, loss: 0.6097363038768857\n",
      "iter: 142, loss: 0.6092135069528297\n",
      "iter: 143, loss: 0.6086915328405899\n",
      "iter: 144, loss: 0.6081703800698963\n",
      "iter: 145, loss: 0.6076500471723972\n",
      "iter: 146, loss: 0.6071305326816634\n",
      "iter: 147, loss: 0.6066118351331908\n",
      "iter: 148, loss: 0.6060939530644055\n",
      "iter: 149, loss: 0.6055768850146659\n",
      "iter: 150, loss: 0.605060629525267\n",
      "iter: 151, loss: 0.6045451851394441\n",
      "iter: 152, loss: 0.6040305504023757\n",
      "iter: 153, loss: 0.6035167238611874\n",
      "iter: 154, loss: 0.6030037040649544\n",
      "iter: 155, loss: 0.602491489564705\n",
      "iter: 156, loss: 0.6019800789134249\n",
      "iter: 157, loss: 0.6014694706660589\n",
      "iter: 158, loss: 0.6009596633795158\n",
      "iter: 159, loss: 0.6004506556126689\n",
      "iter: 160, loss: 0.5999424459263621\n",
      "iter: 161, loss: 0.5994350328834106\n",
      "iter: 162, loss: 0.598928415048605\n",
      "iter: 163, loss: 0.5984225909887141\n",
      "iter: 164, loss: 0.5979175592724875\n",
      "iter: 165, loss: 0.5974133184706579\n",
      "iter: 166, loss: 0.5969098671559456\n",
      "iter: 167, loss: 0.5964072039030603\n",
      "iter: 168, loss: 0.5959053272887023\n",
      "iter: 169, loss: 0.5954042358915683\n",
      "iter: 170, loss: 0.5949039282923512\n",
      "iter: 171, loss: 0.5944044030737445\n",
      "iter: 172, loss: 0.5939056588204437\n",
      "iter: 173, loss: 0.5934076941191495\n",
      "iter: 174, loss: 0.5929105075585708\n",
      "iter: 175, loss: 0.5924140977294239\n",
      "iter: 176, loss: 0.5919184632244406\n",
      "iter: 177, loss: 0.5914236026383649\n",
      "iter: 178, loss: 0.5909295145679584\n",
      "iter: 179, loss: 0.5904361976120018\n",
      "iter: 180, loss: 0.5899436503712974\n",
      "iter: 181, loss: 0.5894518714486703\n",
      "iter: 182, loss: 0.588960859448972\n",
      "iter: 183, loss: 0.5884706129790813\n",
      "iter: 184, loss: 0.5879811306479077\n",
      "iter: 185, loss: 0.5874924110663904\n",
      "iter: 186, loss: 0.587004452847504\n",
      "iter: 187, loss: 0.5865172546062586\n",
      "iter: 188, loss: 0.5860308149597022\n",
      "iter: 189, loss: 0.5855451325269219\n",
      "iter: 190, loss: 0.5850602059290452\n",
      "iter: 191, loss: 0.5845760337892439\n",
      "iter: 192, loss: 0.5840926147327345\n",
      "iter: 193, loss: 0.5836099473867796\n",
      "iter: 194, loss: 0.5831280303806907\n",
      "iter: 195, loss: 0.582646862345828\n",
      "iter: 196, loss: 0.5821664419156041\n",
      "iter: 197, loss: 0.5816867677254839\n",
      "iter: 198, loss: 0.5812078384129876\n",
      "iter: 199, loss: 0.5807296526176906\n",
      "iter: 200, loss: 0.5802522089812259\n",
      "iter: 201, loss: 0.5797755061472868\n",
      "iter: 202, loss: 0.5792995427616242\n",
      "iter: 203, loss: 0.5788243174720528\n",
      "iter: 204, loss: 0.5783498289284491\n",
      "iter: 205, loss: 0.5778760757827537\n",
      "iter: 206, loss: 0.5774030566889738\n",
      "iter: 207, loss: 0.5769307703031814\n",
      "iter: 208, loss: 0.5764592152835174\n",
      "iter: 209, loss: 0.5759883902901922\n",
      "iter: 210, loss: 0.5755182939854842\n",
      "iter: 211, loss: 0.5750489250337447\n",
      "iter: 212, loss: 0.5745802821013961\n",
      "iter: 213, loss: 0.5741123638569341\n",
      "iter: 214, loss: 0.5736451689709292\n",
      "iter: 215, loss: 0.5731786961160252\n",
      "iter: 216, loss: 0.5727129439669434\n",
      "iter: 217, loss: 0.5722479112004804\n",
      "iter: 218, loss: 0.5717835964955117\n",
      "iter: 219, loss: 0.5713199985329901\n",
      "iter: 220, loss: 0.5708571159959486\n",
      "iter: 221, loss: 0.5703949475694992\n",
      "iter: 222, loss: 0.5699334919408338\n",
      "iter: 223, loss: 0.5694727477992274\n",
      "iter: 224, loss: 0.569012713836035\n",
      "iter: 225, loss: 0.5685533887446956\n",
      "iter: 226, loss: 0.5680947712207298\n",
      "iter: 227, loss: 0.5676368599617427\n",
      "iter: 228, loss: 0.5671796536674234\n",
      "iter: 229, loss: 0.5667231510395448\n",
      "iter: 230, loss: 0.5662673507819662\n",
      "iter: 231, loss: 0.5658122516006304\n",
      "iter: 232, loss: 0.5653578522035685\n",
      "iter: 233, loss: 0.5649041513008953\n",
      "iter: 234, loss: 0.5644511476048136\n",
      "iter: 235, loss: 0.5639988398296134\n",
      "iter: 236, loss: 0.5635472266916701\n",
      "iter: 237, loss: 0.5630963069094473\n",
      "iter: 238, loss: 0.5626460792034969\n",
      "iter: 239, loss: 0.5621965422964577\n",
      "iter: 240, loss: 0.5617476949130565\n",
      "iter: 241, loss: 0.5612995357801076\n",
      "iter: 242, loss: 0.5608520636265147\n",
      "iter: 243, loss: 0.5604052771832684\n",
      "iter: 244, loss: 0.5599591751834481\n",
      "iter: 245, loss: 0.5595137563622221\n",
      "iter: 246, loss: 0.5590690194568455\n",
      "iter: 247, loss: 0.5586249632066623\n",
      "iter: 248, loss: 0.5581815863531051\n",
      "iter: 249, loss: 0.5577388876396935\n",
      "iter: 250, loss: 0.557296865812036\n",
      "iter: 251, loss: 0.5568555196178283\n",
      "iter: 252, loss: 0.5564148478068536\n",
      "iter: 253, loss: 0.5559748491309835\n",
      "iter: 254, loss: 0.5555355223441749\n",
      "iter: 255, loss: 0.5550968662024736\n",
      "iter: 256, loss: 0.55465887946401\n",
      "iter: 257, loss: 0.5542215608890032\n",
      "iter: 258, loss: 0.5537849092397564\n",
      "iter: 259, loss: 0.5533489232806591\n",
      "iter: 260, loss: 0.5529136017781864\n",
      "iter: 261, loss: 0.5524789435008972\n",
      "iter: 262, loss: 0.5520449472194361\n",
      "iter: 263, loss: 0.5516116117065306\n",
      "iter: 264, loss: 0.5511789357369932\n",
      "iter: 265, loss: 0.5507469180877176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 266, loss: 0.5503155575376814\n",
      "iter: 267, loss: 0.5498848528679428\n",
      "iter: 268, loss: 0.5494548028616424\n",
      "iter: 269, loss: 0.5490254063040012\n",
      "iter: 270, loss: 0.5485966619823209\n",
      "iter: 271, loss: 0.5481685686859815\n",
      "iter: 272, loss: 0.5477411252064428\n",
      "iter: 273, loss: 0.5473143303372424\n",
      "iter: 274, loss: 0.5468881828739954\n",
      "iter: 275, loss: 0.546462681614393\n",
      "iter: 276, loss: 0.5460378253582033\n",
      "iter: 277, loss: 0.5456136129072693\n",
      "iter: 278, loss: 0.5451900430655079\n",
      "iter: 279, loss: 0.5447671146389093\n",
      "iter: 280, loss: 0.5443448264355379\n",
      "iter: 281, loss: 0.5439231772655283\n",
      "iter: 282, loss: 0.5435021659410865\n",
      "iter: 283, loss: 0.5430817912764888\n",
      "iter: 284, loss: 0.5426620520880803\n",
      "iter: 285, loss: 0.5422429471942742\n",
      "iter: 286, loss: 0.5418244754155513\n",
      "iter: 287, loss: 0.5414066355744582\n",
      "iter: 288, loss: 0.5409894264956062\n",
      "iter: 289, loss: 0.540572847005672\n",
      "iter: 290, loss: 0.5401568959333942\n",
      "iter: 291, loss: 0.5397415721095735\n",
      "iter: 292, loss: 0.5393268743670724\n",
      "iter: 293, loss: 0.5389128015408119\n",
      "iter: 294, loss: 0.5384993524677729\n",
      "iter: 295, loss: 0.5380865259869931\n",
      "iter: 296, loss: 0.5376743209395665\n",
      "iter: 297, loss: 0.537262736168642\n",
      "iter: 298, loss: 0.536851770519424\n",
      "iter: 299, loss: 0.5364414228391671\n",
      "iter: 300, loss: 0.5360316919771795\n",
      "iter: 301, loss: 0.5356225767848182\n",
      "iter: 302, loss: 0.5352140761154899\n",
      "iter: 303, loss: 0.5348061888246494\n",
      "iter: 304, loss: 0.5343989137697958\n",
      "iter: 305, loss: 0.5339922498104748\n",
      "iter: 306, loss: 0.5335861958082758\n",
      "iter: 307, loss: 0.5331807506268289\n",
      "iter: 308, loss: 0.5327759131318067\n",
      "iter: 309, loss: 0.5323716821909205\n",
      "iter: 310, loss: 0.5319680566739194\n",
      "iter: 311, loss: 0.5315650354525893\n",
      "iter: 312, loss: 0.5311626174007507\n",
      "iter: 313, loss: 0.5307608013942587\n",
      "iter: 314, loss: 0.530359586311\n",
      "iter: 315, loss: 0.5299589710308913\n",
      "iter: 316, loss: 0.5295589544358799\n",
      "iter: 317, loss: 0.5291595354099384\n",
      "iter: 318, loss: 0.5287607128390678\n",
      "iter: 319, loss: 0.5283624856112913\n",
      "iter: 320, loss: 0.5279648526166568\n",
      "iter: 321, loss: 0.5275678127472323\n",
      "iter: 322, loss: 0.5271713648971059\n",
      "iter: 323, loss: 0.5267755079623833\n",
      "iter: 324, loss: 0.5263802408411862\n",
      "iter: 325, loss: 0.5259855624336519\n",
      "iter: 326, loss: 0.5255914716419302\n",
      "iter: 327, loss: 0.5251979673701818\n",
      "iter: 328, loss: 0.5248050485245775\n",
      "iter: 329, loss: 0.5244127140132957\n",
      "iter: 330, loss: 0.5240209627465203\n",
      "iter: 331, loss: 0.5236297936364409\n",
      "iter: 332, loss: 0.523239205597248\n",
      "iter: 333, loss: 0.5228491975451344\n",
      "iter: 334, loss: 0.5224597683982907\n",
      "iter: 335, loss: 0.5220709170769053\n",
      "iter: 336, loss: 0.5216826425031619\n",
      "iter: 337, loss: 0.5212949436012374\n",
      "iter: 338, loss: 0.5209078192973008\n",
      "iter: 339, loss: 0.5205212685195104\n",
      "iter: 340, loss: 0.5201352901980135\n",
      "iter: 341, loss: 0.5197498832649413\n",
      "iter: 342, loss: 0.519365046654411\n",
      "iter: 343, loss: 0.5189807793025216\n",
      "iter: 344, loss: 0.5185970801473516\n",
      "iter: 345, loss: 0.5182139481289593\n",
      "iter: 346, loss: 0.5178313821893776\n",
      "iter: 347, loss: 0.5174493812726152\n",
      "iter: 348, loss: 0.5170679443246524\n",
      "iter: 349, loss: 0.5166870702934403\n",
      "iter: 350, loss: 0.5163067581288989\n",
      "iter: 351, loss: 0.5159270067829135\n",
      "iter: 352, loss: 0.5155478152093347\n",
      "iter: 353, loss: 0.5151691823639751\n",
      "iter: 354, loss: 0.5147911072046077\n",
      "iter: 355, loss: 0.5144135886909634\n",
      "iter: 356, loss: 0.51403662578473\n",
      "iter: 357, loss: 0.5136602174495486\n",
      "iter: 358, loss: 0.5132843626510133\n",
      "iter: 359, loss: 0.5129090603566668\n",
      "iter: 360, loss: 0.5125343095360005\n",
      "iter: 361, loss: 0.5121601091604512\n",
      "iter: 362, loss: 0.5117864582033991\n",
      "iter: 363, loss: 0.5114133556401659\n",
      "iter: 364, loss: 0.5110408004480123\n",
      "iter: 365, loss: 0.5106687916061363\n",
      "iter: 366, loss: 0.5102973280956709\n",
      "iter: 367, loss: 0.5099264088996808\n",
      "iter: 368, loss: 0.5095560330031623\n",
      "iter: 369, loss: 0.5091861993930396\n",
      "iter: 370, loss: 0.5088169070581625\n",
      "iter: 371, loss: 0.5084481549893051\n",
      "iter: 372, loss: 0.5080799421791634\n",
      "iter: 373, loss: 0.5077122676223513\n",
      "iter: 374, loss: 0.5073451303154018\n",
      "iter: 375, loss: 0.5069785292567606\n",
      "iter: 376, loss: 0.5066124634467878\n",
      "iter: 377, loss: 0.5062469318877522\n",
      "iter: 378, loss: 0.5058819335838313\n",
      "iter: 379, loss: 0.5055174675411084\n",
      "iter: 380, loss: 0.5051535327675702\n",
      "iter: 381, loss: 0.5047901282731034\n",
      "iter: 382, loss: 0.5044272530694951\n",
      "iter: 383, loss: 0.504064906170427\n",
      "iter: 384, loss: 0.5037030865914756\n",
      "iter: 385, loss: 0.5033417933501094\n",
      "iter: 386, loss: 0.5029810254656857\n",
      "iter: 387, loss: 0.5026207819594487\n",
      "iter: 388, loss: 0.5022610618545277\n",
      "iter: 389, loss: 0.5019018641759335\n",
      "iter: 390, loss: 0.5015431879505574\n",
      "iter: 391, loss: 0.5011850322071666\n",
      "iter: 392, loss: 0.5008273959764054\n",
      "iter: 393, loss: 0.5004702782907888\n",
      "iter: 394, loss: 0.500113678184704\n",
      "iter: 395, loss: 0.4997575946944029\n",
      "iter: 396, loss: 0.49940202685800555\n",
      "iter: 397, loss: 0.4990469737154936\n",
      "iter: 398, loss: 0.4986924343087079\n",
      "iter: 399, loss: 0.49833840768135024\n",
      "iter: 400, loss: 0.4979848928789746\n",
      "iter: 401, loss: 0.4976318889489901\n",
      "iter: 402, loss: 0.49727939494065526\n",
      "iter: 403, loss: 0.4969274099050779\n",
      "iter: 404, loss: 0.49657593289520974\n",
      "iter: 405, loss: 0.4962249629658466\n",
      "iter: 406, loss: 0.49587449917362414\n",
      "iter: 407, loss: 0.4955245405770162\n",
      "iter: 408, loss: 0.4951750862363328\n",
      "iter: 409, loss: 0.4948261352137173\n",
      "iter: 410, loss: 0.4944776865731413\n",
      "iter: 411, loss: 0.4941297393804064\n",
      "iter: 412, loss: 0.4937822927031386\n",
      "iter: 413, loss: 0.49343534561078733\n",
      "iter: 414, loss: 0.4930888971746217\n",
      "iter: 415, loss: 0.49274294646772915\n",
      "iter: 416, loss: 0.492397492565012\n",
      "iter: 417, loss: 0.49205253454318465\n",
      "iter: 418, loss: 0.49170807148077317\n",
      "iter: 419, loss: 0.49136410245810913\n",
      "iter: 420, loss: 0.49102062655732964\n",
      "iter: 421, loss: 0.4906776428623749\n",
      "iter: 422, loss: 0.49033515045898385\n",
      "iter: 423, loss: 0.48999314843469294\n",
      "iter: 424, loss: 0.48965163587883304\n",
      "iter: 425, loss: 0.48931061188252684\n",
      "iter: 426, loss: 0.4889700755386858\n",
      "iter: 427, loss: 0.4886300259420095\n",
      "iter: 428, loss: 0.4882904621889802\n",
      "iter: 429, loss: 0.48795138337786265\n",
      "iter: 430, loss: 0.4876127886086989\n",
      "iter: 431, loss: 0.48727467698330806\n",
      "iter: 432, loss: 0.4869370476052832\n",
      "iter: 433, loss: 0.4865998995799872\n",
      "iter: 434, loss: 0.4862632320145525\n",
      "iter: 435, loss: 0.4859270440178758\n",
      "iter: 436, loss: 0.48559133470061716\n",
      "iter: 437, loss: 0.4852561031751975\n",
      "iter: 438, loss: 0.4849213485557944\n",
      "iter: 439, loss: 0.4845870699583411\n",
      "iter: 440, loss: 0.4842532665005224\n",
      "iter: 441, loss: 0.4839199373017739\n",
      "iter: 442, loss: 0.48358708148327645\n",
      "iter: 443, loss: 0.4832546981679569\n",
      "iter: 444, loss: 0.48292278648048226\n",
      "iter: 445, loss: 0.48259134554725863\n",
      "iter: 446, loss: 0.48226037449642967\n",
      "iter: 447, loss: 0.4819298724578702\n",
      "iter: 448, loss: 0.4815998385631882\n",
      "iter: 449, loss: 0.4812702719457181\n",
      "iter: 450, loss: 0.48094117174052053\n",
      "iter: 451, loss: 0.48061253708437784\n",
      "iter: 452, loss: 0.480284367115794\n",
      "iter: 453, loss: 0.4799566609749888\n",
      "iter: 454, loss: 0.47962941780389734\n",
      "iter: 455, loss: 0.47930263674616574\n",
      "iter: 456, loss: 0.47897631694715137\n",
      "iter: 457, loss: 0.4786504575539146\n",
      "iter: 458, loss: 0.47832505771522243\n",
      "iter: 459, loss: 0.4780001165815419\n",
      "iter: 460, loss: 0.4776756333050375\n",
      "iter: 461, loss: 0.4773516070395701\n",
      "iter: 462, loss: 0.4770280369406932\n",
      "iter: 463, loss: 0.47670492216565\n",
      "iter: 464, loss: 0.4763822618733713\n",
      "iter: 465, loss: 0.47606005522447264\n",
      "iter: 466, loss: 0.47573830138125084\n",
      "iter: 467, loss: 0.4754169995076821\n",
      "iter: 468, loss: 0.4750961487694194\n",
      "iter: 469, loss: 0.4747757483337876\n",
      "iter: 470, loss: 0.4744557973697851\n",
      "iter: 471, loss: 0.474136295048076\n",
      "iter: 472, loss: 0.47381724054099084\n",
      "iter: 473, loss: 0.4734986330225224\n",
      "iter: 474, loss: 0.47318047166832355\n",
      "iter: 475, loss: 0.4728627556557048\n",
      "iter: 476, loss: 0.4725454841636295\n",
      "iter: 477, loss: 0.47222865637271316\n",
      "iter: 478, loss: 0.471912271465221\n",
      "iter: 479, loss: 0.4715963286250636\n",
      "iter: 480, loss: 0.4712808270377941\n",
      "iter: 481, loss: 0.4709657658906075\n",
      "iter: 482, loss: 0.47065114437233535\n",
      "iter: 483, loss: 0.47033696167344524\n",
      "iter: 484, loss: 0.47002321698603583\n",
      "iter: 485, loss: 0.46970990950383684\n",
      "iter: 486, loss: 0.4693970384222028\n",
      "iter: 487, loss: 0.4690846029381132\n",
      "iter: 488, loss: 0.46877260225016876\n",
      "iter: 489, loss: 0.4684610355585883\n",
      "iter: 490, loss: 0.46814990206520607\n",
      "iter: 491, loss: 0.4678392009734689\n",
      "iter: 492, loss: 0.467528931488435\n",
      "iter: 493, loss: 0.4672190928167686\n",
      "iter: 494, loss: 0.46690968416673856\n",
      "iter: 495, loss: 0.46660070474821597\n",
      "iter: 496, loss: 0.46629215377267047\n",
      "iter: 497, loss: 0.4659840304531685\n",
      "iter: 498, loss: 0.46567633400436814\n",
      "iter: 499, loss: 0.4653690636425212\n",
      "iter: 500, loss: 0.46506221858546426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 501, loss: 0.46475579805262063\n",
      "iter: 502, loss: 0.4644498012649956\n",
      "iter: 503, loss: 0.4641442274451741\n",
      "iter: 504, loss: 0.4638390758173169\n",
      "iter: 505, loss: 0.46353434560716034\n",
      "iter: 506, loss: 0.4632300360420109\n",
      "iter: 507, loss: 0.4629261463507433\n",
      "iter: 508, loss: 0.4626226757637987\n",
      "iter: 509, loss: 0.4623196235131804\n",
      "iter: 510, loss: 0.4620169888324526\n",
      "iter: 511, loss: 0.46171477095673585\n",
      "iter: 512, loss: 0.46141296912270596\n",
      "iter: 513, loss: 0.4611115825685901\n",
      "iter: 514, loss: 0.46081061053416456\n",
      "iter: 515, loss: 0.4605100522607524\n",
      "iter: 516, loss: 0.46020990699121905\n",
      "iter: 517, loss: 0.4599101739699713\n",
      "iter: 518, loss: 0.45961085244295397\n",
      "iter: 519, loss: 0.45931194165764666\n",
      "iter: 520, loss: 0.4590134408630612\n",
      "iter: 521, loss: 0.45871534930973906\n",
      "iter: 522, loss: 0.4584176662497495\n",
      "iter: 523, loss: 0.45812039093668416\n",
      "iter: 524, loss: 0.45782352262565723\n",
      "iter: 525, loss: 0.4575270605733007\n",
      "iter: 526, loss: 0.457231004037763\n",
      "iter: 527, loss: 0.45693535227870485\n",
      "iter: 528, loss: 0.4566401045572968\n",
      "iter: 529, loss: 0.4563452601362188\n",
      "iter: 530, loss: 0.4560508182796535\n",
      "iter: 531, loss: 0.4557567782532861\n",
      "iter: 532, loss: 0.4554631393243007\n",
      "iter: 533, loss: 0.4551699007613786\n",
      "iter: 534, loss: 0.4548770618346943\n",
      "iter: 535, loss: 0.4545846218159125\n",
      "iter: 536, loss: 0.454292579978187\n",
      "iter: 537, loss: 0.4540009355961569\n",
      "iter: 538, loss: 0.453709687945943\n",
      "iter: 539, loss: 0.4534188363051468\n",
      "iter: 540, loss: 0.45312837995284755\n",
      "iter: 541, loss: 0.4528383181695962\n",
      "iter: 542, loss: 0.4525486502374185\n",
      "iter: 543, loss: 0.452259375439807\n",
      "iter: 544, loss: 0.45197049306172155\n",
      "iter: 545, loss: 0.4516820023895842\n",
      "iter: 546, loss: 0.45139390271127816\n",
      "iter: 547, loss: 0.45110619331614565\n",
      "iter: 548, loss: 0.45081887349498245\n",
      "iter: 549, loss: 0.450531942540037\n",
      "iter: 550, loss: 0.450245399745008\n",
      "iter: 551, loss: 0.44995924440504065\n",
      "iter: 552, loss: 0.4496734758167245\n",
      "iter: 553, loss: 0.44938809327809054\n",
      "iter: 554, loss: 0.44910309608860915\n",
      "iter: 555, loss: 0.4488184835491851\n",
      "iter: 556, loss: 0.4485342549621578\n",
      "iter: 557, loss: 0.44825040963129714\n",
      "iter: 558, loss: 0.4479669468618001\n",
      "iter: 559, loss: 0.4476838659602897\n",
      "iter: 560, loss: 0.44740116623481013\n",
      "iter: 561, loss: 0.447118846994827\n",
      "iter: 562, loss: 0.4468369075512216\n",
      "iter: 563, loss: 0.4465553472162888\n",
      "iter: 564, loss: 0.44627416530373715\n",
      "iter: 565, loss: 0.445993361128682\n",
      "iter: 566, loss: 0.445712934007646\n",
      "iter: 567, loss: 0.44543288325855535\n",
      "iter: 568, loss: 0.4451532082007357\n",
      "iter: 569, loss: 0.4448739081549115\n",
      "iter: 570, loss: 0.44459498244320306\n",
      "iter: 571, loss: 0.44431643038912255\n",
      "iter: 572, loss: 0.44403825131757196\n",
      "iter: 573, loss: 0.443760444554841\n",
      "iter: 574, loss: 0.4434830094286042\n",
      "iter: 575, loss: 0.44320594526791623\n",
      "iter: 576, loss: 0.4429292514032136\n",
      "iter: 577, loss: 0.44265292716630694\n",
      "iter: 578, loss: 0.4423769718903822\n",
      "iter: 579, loss: 0.44210138490999545\n",
      "iter: 580, loss: 0.4418261655610717\n",
      "iter: 581, loss: 0.44155131318090207\n",
      "iter: 582, loss: 0.4412768271081394\n",
      "iter: 583, loss: 0.4410027066827991\n",
      "iter: 584, loss: 0.4407289512462523\n",
      "iter: 585, loss: 0.44045556014122683\n",
      "iter: 586, loss: 0.440182532711802\n",
      "iter: 587, loss: 0.43990986830340695\n",
      "iter: 588, loss: 0.43963756626281847\n",
      "iter: 589, loss: 0.43936562593815714\n",
      "iter: 590, loss: 0.4390940466788865\n",
      "iter: 591, loss: 0.4388228278358081\n",
      "iter: 592, loss: 0.43855196876106034\n",
      "iter: 593, loss: 0.43828146880811597\n",
      "iter: 594, loss: 0.4380113273317783\n",
      "iter: 595, loss: 0.43774154368817997\n",
      "iter: 596, loss: 0.4374721172347788\n",
      "iter: 597, loss: 0.4372030473303569\n",
      "iter: 598, loss: 0.43693433333501636\n",
      "iter: 599, loss: 0.43666597461017775\n",
      "iter: 600, loss: 0.4363979705185767\n",
      "iter: 601, loss: 0.43613032042426225\n",
      "iter: 602, loss: 0.4358630236925932\n",
      "iter: 603, loss: 0.4355960796902363\n",
      "iter: 604, loss: 0.435329487785163\n",
      "iter: 605, loss: 0.43506324734664736\n",
      "iter: 606, loss: 0.43479735774526296\n",
      "iter: 607, loss: 0.43453181835288146\n",
      "iter: 608, loss: 0.43426662854266757\n",
      "iter: 609, loss: 0.43400178768907965\n",
      "iter: 610, loss: 0.4337372951678642\n",
      "iter: 611, loss: 0.4334731503560543\n",
      "iter: 612, loss: 0.43320935263196975\n",
      "iter: 613, loss: 0.4329459013752078\n",
      "iter: 614, loss: 0.432682795966649\n",
      "iter: 615, loss: 0.4324200357884474\n",
      "iter: 616, loss: 0.4321576202240328\n",
      "iter: 617, loss: 0.4318955486581046\n",
      "iter: 618, loss: 0.4316338204766322\n",
      "iter: 619, loss: 0.4313724350668515\n",
      "iter: 620, loss: 0.43111139181726016\n",
      "iter: 621, loss: 0.4308506901176191\n",
      "iter: 622, loss: 0.43059032935894626\n",
      "iter: 623, loss: 0.43033030893351615\n",
      "iter: 624, loss: 0.43007062823485603\n",
      "iter: 625, loss: 0.42981128665774543\n",
      "iter: 626, loss: 0.4295522835982101\n",
      "iter: 627, loss: 0.42929361845352365\n",
      "iter: 628, loss: 0.42903529062220125\n",
      "iter: 629, loss: 0.42877729950399995\n",
      "iter: 630, loss: 0.42851964449991437\n",
      "iter: 631, loss: 0.4282623250121743\n",
      "iter: 632, loss: 0.42800534044424376\n",
      "iter: 633, loss: 0.42774869020081646\n",
      "iter: 634, loss: 0.42749237368781445\n",
      "iter: 635, loss: 0.42723639031238575\n",
      "iter: 636, loss: 0.42698073948289994\n",
      "iter: 637, loss: 0.42672542060894936\n",
      "iter: 638, loss: 0.4264704331013424\n",
      "iter: 639, loss: 0.426215776372104\n",
      "iter: 640, loss: 0.4259614498344716\n",
      "iter: 641, loss: 0.42570745290289375\n",
      "iter: 642, loss: 0.4254537849930265\n",
      "iter: 643, loss: 0.4252004455217314\n",
      "iter: 644, loss: 0.4249474339070737\n",
      "iter: 645, loss: 0.42469474956831843\n",
      "iter: 646, loss: 0.4244423919259293\n",
      "iter: 647, loss: 0.42419036040156544\n",
      "iter: 648, loss: 0.4239386544180788\n",
      "iter: 649, loss: 0.4236872733995125\n",
      "iter: 650, loss: 0.423436216771098\n",
      "iter: 651, loss: 0.4231854839592517\n",
      "iter: 652, loss: 0.4229350743915737\n",
      "iter: 653, loss: 0.42268498749684574\n",
      "iter: 654, loss: 0.4224352227050266\n",
      "iter: 655, loss: 0.42218577944725205\n",
      "iter: 656, loss: 0.4219366571558308\n",
      "iter: 657, loss: 0.42168785526424274\n",
      "iter: 658, loss: 0.4214393732071366\n",
      "iter: 659, loss: 0.42119121042032737\n",
      "iter: 660, loss: 0.4209433663407929\n",
      "iter: 661, loss: 0.42069584040667357\n",
      "iter: 662, loss: 0.4204486320572682\n",
      "iter: 663, loss: 0.4202017407330322\n",
      "iter: 664, loss: 0.4199551658755745\n",
      "iter: 665, loss: 0.41970890692765617\n",
      "iter: 666, loss: 0.4194629633331881\n",
      "iter: 667, loss: 0.41921733453722715\n",
      "iter: 668, loss: 0.4189720199859747\n",
      "iter: 669, loss: 0.418727019126775\n",
      "iter: 670, loss: 0.41848233140811103\n",
      "iter: 671, loss: 0.4182379562796039\n",
      "iter: 672, loss: 0.41799389319200875\n",
      "iter: 673, loss: 0.41775014159721385\n",
      "iter: 674, loss: 0.4175067009482373\n",
      "iter: 675, loss: 0.4172635706992256\n",
      "iter: 676, loss: 0.41702075030544966\n",
      "iter: 677, loss: 0.41677823922330376\n",
      "iter: 678, loss: 0.4165360369103036\n",
      "iter: 679, loss: 0.416294142825082\n",
      "iter: 680, loss: 0.41605255642738875\n",
      "iter: 681, loss: 0.4158112771780866\n",
      "iter: 682, loss: 0.41557030453914945\n",
      "iter: 683, loss: 0.41532963797366074\n",
      "iter: 684, loss: 0.4150892769458099\n",
      "iter: 685, loss: 0.414849220920891\n",
      "iter: 686, loss: 0.4146094693652998\n",
      "iter: 687, loss: 0.4143700217465313\n",
      "iter: 688, loss: 0.41413087753317823\n",
      "iter: 689, loss: 0.4138920361949281\n",
      "iter: 690, loss: 0.41365349720256156\n",
      "iter: 691, loss: 0.4134152600279478\n",
      "iter: 692, loss: 0.4131773241440455\n",
      "iter: 693, loss: 0.4129396890248989\n",
      "iter: 694, loss: 0.4127023541456351\n",
      "iter: 695, loss: 0.4124653189824627\n",
      "iter: 696, loss: 0.4122285830126685\n",
      "iter: 697, loss: 0.4119921457146163\n",
      "iter: 698, loss: 0.4117560065677439\n",
      "iter: 699, loss: 0.41152016505256084\n",
      "iter: 700, loss: 0.4112846206506466\n",
      "iter: 701, loss: 0.41104937284464765\n",
      "iter: 702, loss: 0.4108144211182752\n",
      "iter: 703, loss: 0.41057976495630455\n",
      "iter: 704, loss: 0.41034540384457047\n",
      "iter: 705, loss: 0.41011133726996624\n",
      "iter: 706, loss: 0.40987756472044085\n",
      "iter: 707, loss: 0.4096440856849978\n",
      "iter: 708, loss: 0.4094108996536911\n",
      "iter: 709, loss: 0.4091780061176247\n",
      "iter: 710, loss: 0.40894540456894996\n",
      "iter: 711, loss: 0.40871309450086174\n",
      "iter: 712, loss: 0.4084810754075987\n",
      "iter: 713, loss: 0.40824934678443925\n",
      "iter: 714, loss: 0.4080179081276999\n",
      "iter: 715, loss: 0.40778675893473265\n",
      "iter: 716, loss: 0.4075558987039239\n",
      "iter: 717, loss: 0.407325326934691\n",
      "iter: 718, loss: 0.407095043127481\n",
      "iter: 719, loss: 0.40686504678376734\n",
      "iter: 720, loss: 0.4066353374060482\n",
      "iter: 721, loss: 0.40640591449784536\n",
      "iter: 722, loss: 0.40617677756369974\n",
      "iter: 723, loss: 0.40594792610917124\n",
      "iter: 724, loss: 0.4057193596408359\n",
      "iter: 725, loss: 0.40549107766628256\n",
      "iter: 726, loss: 0.4052630796941132\n",
      "iter: 727, loss: 0.4050353652339384\n",
      "iter: 728, loss: 0.404807933796376\n",
      "iter: 729, loss: 0.4045807848930494\n",
      "iter: 730, loss: 0.40435391803658455\n",
      "iter: 731, loss: 0.4041273327406084\n",
      "iter: 732, loss: 0.40390102851974696\n",
      "iter: 733, loss: 0.40367500488962205\n",
      "iter: 734, loss: 0.40344926136685033\n",
      "iter: 735, loss: 0.40322379746904025\n",
      "iter: 736, loss: 0.4029986127147911\n",
      "iter: 737, loss: 0.4027737066236885\n",
      "iter: 738, loss: 0.40254907871630635\n",
      "iter: 739, loss: 0.40232472851419954\n",
      "iter: 740, loss: 0.40210065553990637\n",
      "iter: 741, loss: 0.40187685931694406\n",
      "iter: 742, loss: 0.40165333936980685\n",
      "iter: 743, loss: 0.40143009522396406\n",
      "iter: 744, loss: 0.4012071264058584\n",
      "iter: 745, loss: 0.40098443244290377\n",
      "iter: 746, loss: 0.4007620128634825\n",
      "iter: 747, loss: 0.40053986719694373\n",
      "iter: 748, loss: 0.4003179949736011\n",
      "iter: 749, loss: 0.4000963957247317\n",
      "iter: 750, loss: 0.3998750689825721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 751, loss: 0.39965401428031755\n",
      "iter: 752, loss: 0.39943323115211987\n",
      "iter: 753, loss: 0.39921271913308504\n",
      "iter: 754, loss: 0.398992477759271\n",
      "iter: 755, loss: 0.39877250656768587\n",
      "iter: 756, loss: 0.39855280509628604\n",
      "iter: 757, loss: 0.39833337288397375\n",
      "iter: 758, loss: 0.398114209470595\n",
      "iter: 759, loss: 0.39789531439693787\n",
      "iter: 760, loss: 0.3976766872047301\n",
      "iter: 761, loss: 0.39745832743663756\n",
      "iter: 762, loss: 0.3972402346362614\n",
      "iter: 763, loss: 0.39702240834813657\n",
      "iter: 764, loss: 0.39680484811773015\n",
      "iter: 765, loss: 0.39658755349143765\n",
      "iter: 766, loss: 0.396370524016584\n",
      "iter: 767, loss: 0.39615375924141843\n",
      "iter: 768, loss: 0.3959372587151138\n",
      "iter: 769, loss: 0.39572102198776493\n",
      "iter: 770, loss: 0.395505048610386\n",
      "iter: 771, loss: 0.3952893381349092\n",
      "iter: 772, loss: 0.39507389011418076\n",
      "iter: 773, loss: 0.39485870410196283\n",
      "iter: 774, loss: 0.3946437796529272\n",
      "iter: 775, loss: 0.39442911632265554\n",
      "iter: 776, loss: 0.39421471366763755\n",
      "iter: 777, loss: 0.39400057124526855\n",
      "iter: 778, loss: 0.39378668861384725\n",
      "iter: 779, loss: 0.3935730653325736\n",
      "iter: 780, loss: 0.39335970096154776\n",
      "iter: 781, loss: 0.3931465950617673\n",
      "iter: 782, loss: 0.39293374719512625\n",
      "iter: 783, loss: 0.39272115692441095\n",
      "iter: 784, loss: 0.3925088238133006\n",
      "iter: 785, loss: 0.3922967474263644\n",
      "iter: 786, loss: 0.3920849273290588\n",
      "iter: 787, loss: 0.3918733630877267\n",
      "iter: 788, loss: 0.3916620542695948\n",
      "iter: 789, loss: 0.3914510004427716\n",
      "iter: 790, loss: 0.3912402011762466\n",
      "iter: 791, loss: 0.3910296560398866\n",
      "iter: 792, loss: 0.39081936460443545\n",
      "iter: 793, loss: 0.3906093264415105\n",
      "iter: 794, loss: 0.39039954112360276\n",
      "iter: 795, loss: 0.39019000822407274\n",
      "iter: 796, loss: 0.3899807273171499\n",
      "iter: 797, loss: 0.3897716979779306\n",
      "iter: 798, loss: 0.38956291978237617\n",
      "iter: 799, loss: 0.38935439230731034\n",
      "iter: 800, loss: 0.3891461151304185\n",
      "iter: 801, loss: 0.38893808783024425\n",
      "iter: 802, loss: 0.3887303099861895\n",
      "iter: 803, loss: 0.38852278117851086\n",
      "iter: 804, loss: 0.38831550098831874\n",
      "iter: 805, loss: 0.38810846899757495\n",
      "iter: 806, loss: 0.38790168478909137\n",
      "iter: 807, loss: 0.3876951479465275\n",
      "iter: 808, loss: 0.3874888580543886\n",
      "iter: 809, loss: 0.3872828146980244\n",
      "iter: 810, loss: 0.38707701746362716\n",
      "iter: 811, loss: 0.386871465938229\n",
      "iter: 812, loss: 0.3866661597097014\n",
      "iter: 813, loss: 0.3864610983667515\n",
      "iter: 814, loss: 0.38625628149892255\n",
      "iter: 815, loss: 0.38605170869658983\n",
      "iter: 816, loss: 0.3858473795509604\n",
      "iter: 817, loss: 0.3856432936540707\n",
      "iter: 818, loss: 0.3854394505987843\n",
      "iter: 819, loss: 0.38523584997879157\n",
      "iter: 820, loss: 0.38503249138860535\n",
      "iter: 821, loss: 0.3848293744235615\n",
      "iter: 822, loss: 0.3846264986798164\n",
      "iter: 823, loss: 0.3844238637543442\n",
      "iter: 824, loss: 0.38422146924493605\n",
      "iter: 825, loss: 0.384019314750198\n",
      "iter: 826, loss: 0.38381739986954927\n",
      "iter: 827, loss: 0.38361572420322027\n",
      "iter: 828, loss: 0.38341428735225086\n",
      "iter: 829, loss: 0.3832130889184887\n",
      "iter: 830, loss: 0.3830121285045875\n",
      "iter: 831, loss: 0.3828114057140047\n",
      "iter: 832, loss: 0.3826109201510007\n",
      "iter: 833, loss: 0.3824106714206364\n",
      "iter: 834, loss: 0.3822106591287707\n",
      "iter: 835, loss: 0.3820108828820606\n",
      "iter: 836, loss: 0.3818113422879581\n",
      "iter: 837, loss: 0.3816120369547089\n",
      "iter: 838, loss: 0.38141296649134987\n",
      "iter: 839, loss: 0.3812141305077087\n",
      "iter: 840, loss: 0.38101552861440086\n",
      "iter: 841, loss: 0.3808171604228287\n",
      "iter: 842, loss: 0.38061902554517973\n",
      "iter: 843, loss: 0.38042112359442326\n",
      "iter: 844, loss: 0.3802234541843114\n",
      "iter: 845, loss: 0.38002601692937554\n",
      "iter: 846, loss: 0.37982881144492425\n",
      "iter: 847, loss: 0.3796318373470434\n",
      "iter: 848, loss: 0.3794350942525919\n",
      "iter: 849, loss: 0.37923858177920317\n",
      "iter: 850, loss: 0.3790422995452803\n",
      "iter: 851, loss: 0.37884624716999626\n",
      "iter: 852, loss: 0.3786504242732919\n",
      "iter: 853, loss: 0.3784548304758735\n",
      "iter: 854, loss: 0.37825946539921196\n",
      "iter: 855, loss: 0.37806432866554035\n",
      "iter: 856, loss: 0.3778694198978535\n",
      "iter: 857, loss: 0.37767473871990426\n",
      "iter: 858, loss: 0.37748028475620343\n",
      "iter: 859, loss: 0.3772860576320182\n",
      "iter: 860, loss: 0.37709205697336917\n",
      "iter: 861, loss: 0.3768982824070301\n",
      "iter: 862, loss: 0.37670473356052464\n",
      "iter: 863, loss: 0.37651141006212635\n",
      "iter: 864, loss: 0.37631831154085693\n",
      "iter: 865, loss: 0.37612543762648165\n",
      "iter: 866, loss: 0.3759327879495122\n",
      "iter: 867, loss: 0.375740362141202\n",
      "iter: 868, loss: 0.37554815983354545\n",
      "iter: 869, loss: 0.37535618065927584\n",
      "iter: 870, loss: 0.3751644242518647\n",
      "iter: 871, loss: 0.3749728902455191\n",
      "iter: 872, loss: 0.37478157827518044\n",
      "iter: 873, loss: 0.37459048797652317\n",
      "iter: 874, loss: 0.3743996189859526\n",
      "iter: 875, loss: 0.3742089709406036\n",
      "iter: 876, loss: 0.3740185434783389\n",
      "iter: 877, loss: 0.3738283362377474\n",
      "iter: 878, loss: 0.3736383488581429\n",
      "iter: 879, loss: 0.3734485809795618\n",
      "iter: 880, loss: 0.3732590322427619\n",
      "iter: 881, loss: 0.3730697022892221\n",
      "iter: 882, loss: 0.3728805907611374\n",
      "iter: 883, loss: 0.37269169730142115\n",
      "iter: 884, loss: 0.372503021553701\n",
      "iter: 885, loss: 0.3723145631623185\n",
      "iter: 886, loss: 0.3721263217723266\n",
      "iter: 887, loss: 0.3719382970294888\n",
      "iter: 888, loss: 0.371750488580277\n",
      "iter: 889, loss: 0.3715628960718712\n",
      "iter: 890, loss: 0.37137551915215583\n",
      "iter: 891, loss: 0.37118835746972023\n",
      "iter: 892, loss: 0.3710014106738555\n",
      "iter: 893, loss: 0.37081467841455373\n",
      "iter: 894, loss: 0.37062816034250745\n",
      "iter: 895, loss: 0.3704418561091054\n",
      "iter: 896, loss: 0.37025576536643373\n",
      "iter: 897, loss: 0.37006988776727257\n",
      "iter: 898, loss: 0.3698842229650957\n",
      "iter: 899, loss: 0.3696987706140684\n",
      "iter: 900, loss: 0.36951353036904566\n",
      "iter: 901, loss: 0.3693285018855717\n",
      "iter: 902, loss: 0.3691436848198771\n",
      "iter: 903, loss: 0.36895907882887796\n",
      "iter: 904, loss: 0.3687746835701755\n",
      "iter: 905, loss: 0.36859049870205185\n",
      "iter: 906, loss: 0.368406523883471\n",
      "iter: 907, loss: 0.3682227587740765\n",
      "iter: 908, loss: 0.3680392030341891\n",
      "iter: 909, loss: 0.3678558563248063\n",
      "iter: 910, loss: 0.36767271830760073\n",
      "iter: 911, loss: 0.3674897886449188\n",
      "iter: 912, loss: 0.36730706699977744\n",
      "iter: 913, loss: 0.36712455303586566\n",
      "iter: 914, loss: 0.3669422464175407\n",
      "iter: 915, loss: 0.3667601468098269\n",
      "iter: 916, loss: 0.366578253878415\n",
      "iter: 917, loss: 0.36639656728966097\n",
      "iter: 918, loss: 0.3662150867105819\n",
      "iter: 919, loss: 0.36603381180885836\n",
      "iter: 920, loss: 0.3658527422528301\n",
      "iter: 921, loss: 0.3656718777114956\n",
      "iter: 922, loss: 0.36549121785451055\n",
      "iter: 923, loss: 0.3653107623521862\n",
      "iter: 924, loss: 0.3651305108754886\n",
      "iter: 925, loss: 0.36495046309603607\n",
      "iter: 926, loss: 0.3647706186860978\n",
      "iter: 927, loss: 0.3645909773185945\n",
      "iter: 928, loss: 0.36441153866709347\n",
      "iter: 929, loss: 0.3642323024058101\n",
      "iter: 930, loss: 0.3640532682096059\n",
      "iter: 931, loss: 0.3638744357539847\n",
      "iter: 932, loss: 0.36369580471509494\n",
      "iter: 933, loss: 0.36351737476972523\n",
      "iter: 934, loss: 0.36333914559530456\n",
      "iter: 935, loss: 0.36316111686989955\n",
      "iter: 936, loss: 0.3629832882722155\n",
      "iter: 937, loss: 0.362805659481592\n",
      "iter: 938, loss: 0.36262823017800305\n",
      "iter: 939, loss: 0.36245100004205516\n",
      "iter: 940, loss: 0.3622739687549874\n",
      "iter: 941, loss: 0.362097135998668\n",
      "iter: 942, loss: 0.3619205014555934\n",
      "iter: 943, loss: 0.3617440648088883\n",
      "iter: 944, loss: 0.3615678257423024\n",
      "iter: 945, loss: 0.36139178394021054\n",
      "iter: 946, loss: 0.36121593908760935\n",
      "iter: 947, loss: 0.3610402908701182\n",
      "iter: 948, loss: 0.3608648389739769\n",
      "iter: 949, loss: 0.36068958308604304\n",
      "iter: 950, loss: 0.36051452289379293\n",
      "iter: 951, loss: 0.36033965808531837\n",
      "iter: 952, loss: 0.3601649883493259\n",
      "iter: 953, loss: 0.3599905133751362\n",
      "iter: 954, loss: 0.359816232852681\n",
      "iter: 955, loss: 0.3596421464725028\n",
      "iter: 956, loss: 0.3594682539257551\n",
      "iter: 957, loss: 0.3592945549041978\n",
      "iter: 958, loss: 0.35912104910019704\n",
      "iter: 959, loss: 0.3589477362067257\n",
      "iter: 960, loss: 0.3587746159173601\n",
      "iter: 961, loss: 0.35860168792627917\n",
      "iter: 962, loss: 0.3584289519282626\n",
      "iter: 963, loss: 0.35825640761869065\n",
      "iter: 964, loss: 0.35808405469354215\n",
      "iter: 965, loss: 0.3579118928493934\n",
      "iter: 966, loss: 0.35773992178341535\n",
      "iter: 967, loss: 0.357568141193376\n",
      "iter: 968, loss: 0.3573965507776346\n",
      "iter: 969, loss: 0.35722515023514306\n",
      "iter: 970, loss: 0.35705393926544443\n",
      "iter: 971, loss: 0.35688291756867047\n",
      "iter: 972, loss: 0.35671208484554157\n",
      "iter: 973, loss: 0.35654144079736466\n",
      "iter: 974, loss: 0.3563709851260324\n",
      "iter: 975, loss: 0.3562007175340212\n",
      "iter: 976, loss: 0.35603063772439064\n",
      "iter: 977, loss: 0.35586074540078216\n",
      "iter: 978, loss: 0.35569104026741677\n",
      "iter: 979, loss: 0.35552152202909526\n",
      "iter: 980, loss: 0.3553521903911958\n",
      "iter: 981, loss: 0.35518304505967313\n",
      "iter: 982, loss: 0.355014085741057\n",
      "iter: 983, loss: 0.3548453121424512\n",
      "iter: 984, loss: 0.35467672397153266\n",
      "iter: 985, loss: 0.3545083209365487\n",
      "iter: 986, loss: 0.35434010274631755\n",
      "iter: 987, loss: 0.35417206911022553\n",
      "iter: 988, loss: 0.35400421973822765\n",
      "iter: 989, loss: 0.3538365543408443\n",
      "iter: 990, loss: 0.35366907262916136\n",
      "iter: 991, loss: 0.35350177431482804\n",
      "iter: 992, loss: 0.35333465911005696\n",
      "iter: 993, loss: 0.3531677267276217\n",
      "iter: 994, loss: 0.35300097688085535\n",
      "iter: 995, loss: 0.35283440928365095\n",
      "iter: 996, loss: 0.35266802365045813\n",
      "iter: 997, loss: 0.3525018196962836\n",
      "iter: 998, loss: 0.35233579713668894\n",
      "iter: 999, loss: 0.3521699556877894\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:39:19.507058Z",
     "start_time": "2019-01-11T20:39:19.394090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets plot our decision boundary with the data\n",
    "plt.plot(X0[:,0],X0[:,1],'o',label='Class 0')\n",
    "plt.plot(X1[:,0],X1[:,1],'o',label='Class 1')\n",
    "x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "probs = np.array(predict_proba(grid,theta)).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, [0.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:40:03.059184Z",
     "start_time": "2019-01-11T20:39:35.765743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.39 s ± 12.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:40:06.623285Z",
     "start_time": "2019-01-11T20:40:03.104095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 114.69 MiB, increment: 0.15 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: Get Rid of Basic Loops\n",
    "You will get rid of the loops in the helper functions `sigmoid`, `loss`, `predict`, and `accuracy`, and some of the loops in `fit`\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "    \n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "    \n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            for j in range(X.shape[1]):\n",
    "                grad += X[i,j]*theta[j]\n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            grad_ele = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                grad_ele += X[i,j]*(h-y)[i]\n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 to 2 LINES\n",
    "        #HINT: FIRST CAST `gradient` TO AN ARRAY, THEN DIVIDE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        grad = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            grad += X[i,j]*theta[j]\n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2: Get Rid of Inner Loops\n",
    "\n",
    "Get rid of the inner loops in `fit` and `predict_proba`\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        z = []\n",
    "        for i in range(X.shape[0]):\n",
    "            grad = 0\n",
    "            \n",
    "            #WRITE CODE HERE\n",
    "            #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "\n",
    "            ####################################\n",
    "            \n",
    "            z.append(grad)\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        gradient = []\n",
    "        for j in range(X.shape[1]):\n",
    "            \n",
    "            #WRITE CODE HERE\n",
    "            #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "            #HINT: TO ACCESS THE jth COLUMN OF X WRITE X[:,j]\n",
    "\n",
    "            ####################################\n",
    "            \n",
    "            gradient.append(grad_ele)\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    z = []\n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS IS THE SAME CODE AS FOR THE FIRST INNER LOOP IN `fit`\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        z.append(grad)\n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#You should get .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 3: No Loops (Except in Iterations)\n",
    "You will get ride of the outerloop in `fit` and `predict proba`\n",
    "This will confer a huge time save.\n",
    "\n",
    "Write code where it says \n",
    "`#WRITE CODE HERE`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        \n",
    "        #WRITE CODE HERE\n",
    "        #NOTE: THIS CAN BE DONE IN 1 LINE\n",
    "        #HINT: You'll have to use the axis argument in \n",
    "        #https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.sum.html\n",
    "\n",
    "        ####################################\n",
    "\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        #WRITE CODE HERE\n",
    "        #HINT: YOU'LL AGAIN HAVE TO USE THE AXIS ARGUMENT IN SUM\n",
    "        #HINT2: TO TURN AN (n,) array to a (n,1) array use <ARR>[:,None]\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector    \n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS IS THE SAME CODE AS FOR THE FIRST LOOP IN `fit`\n",
    "\n",
    "    ####################################\n",
    "        \n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4: More Time Saves (Bonus)\n",
    "\n",
    "For a Bonus 2 points, replace some of the sums with dot products. This is even faster.\n",
    "\n",
    "Write code where it says \n",
    "`#REPLACE WITH DOT PRODUCT`\n",
    "\n",
    "Copy the code you wrote in iteration 1 where it says\n",
    "`#COPY CODE FROM ITERATION 1 HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Add the intercept to our dataset\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    #KEEP THIS LINE\n",
    "    z = np.array(z)\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def loss(h, y):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "def fit(X, y,num_iter,alpha,verbose=True):\n",
    "    \n",
    "    #add intercept\n",
    "    X = add_intercept(X)\n",
    "        \n",
    "    # weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    #Note: we cannot remove this loop at there is a temporal relation here\n",
    "    for it in range(num_iter):\n",
    "        \n",
    "        #Create z vector\n",
    "        \n",
    "        #REPLACE WITH DOT PRODUCT\n",
    "        #HINT: A @ B denotes the dot product of A and B\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        #Create gradient array\n",
    "        #REPLACE WITH DOT PRODUCT\n",
    "        #HINT: TO TAKE THE TRANPOSE OF MATRIX A, USE A.T\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Divide all elements by number of examples\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Update theta\n",
    "        #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "        #Print if verbose\n",
    "        if verbose:\n",
    "            print('iter: {}, loss: {}'.format(it,loss(h,y)))\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def predict_proba(X,theta):\n",
    "    X = add_intercept(X)\n",
    "    \n",
    "    #Create z vector\n",
    "    #WRITE CODE HERE\n",
    "    #NOTE: THIS IS THE SAME CODE AS FOR THE DOT PRODUCT IN `fit`\n",
    "\n",
    "    ####################################\n",
    "        \n",
    "    return sigmoid(z)\n",
    "    \n",
    "def predict(X,theta):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    #COPY CODE FROM ITERATION 1 HERE\n",
    "\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check the accuracy after 1000 iterations\n",
    "theta = fit(X,y,num_iter,alpha)\n",
    "y_pred = predict(X,theta)\n",
    "print(accuracy(y,y_pred))\n",
    "#Should be .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "#fit using the fit method, turn verbose to false\n",
    "theta = fit(X,y,num_iter,alpha,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The way the pros implement it (No code to write, just observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:42:09.094471Z",
     "start_time": "2019-01-11T20:42:08.959188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X)\n",
    "#Check accuracy\n",
    "accuracy(y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:42:12.813814Z",
     "start_time": "2019-01-11T20:42:10.656069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 µs ± 715 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Lets time it\n",
    "clf = LogisticRegression()\n",
    "%timeit clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T20:42:14.774645Z",
     "start_time": "2019-01-11T20:42:14.596944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 118.14 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "#Lets mem it\n",
    "clf = LogisticRegression()\n",
    "%memit clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
